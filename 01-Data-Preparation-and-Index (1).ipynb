{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f741ef26-6aa0-46ff-a654-3e14c2b0d1ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Databricks-BR/genai_hackathon/main/images/head_genai_hackathon.gif\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a7ae8f9-feeb-44a8-8d00-e738d5add048",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Databricks-BR/genai_hackathon/main/images/head_titulo.png\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f9c6b8f-dedb-4f68-8d55-2162908a1462",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Título:  Interação medicamentosa\n",
    "\n",
    "| ITEM | DESCRIÇÃO |\n",
    "| -- | -- |\n",
    "| Indústria: | Saúde \n",
    "| Departamento: | Farmaceutico\n",
    "| Tipo de Solução: | Assistente GenAI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83e1fdbe-d818-4499-aaab-3e5857fb6f98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "<img src=\"https://raw.githubusercontent.com/Databricks-BR/genai_hackathon/main/images/head_grupo.png\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33dcbeca-6ff8-4876-bb22-dc328533b077",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### GRUPO 02 - ZeDroginha\n",
    "\n",
    "| # | Nome do Integrante | Empresa | e-mail |\n",
    "| -- | -- | -- | -- |\n",
    "| 1 | Rodrigo Toledo | RD | rpptoledo@rd.com.br |\n",
    "| 2 | Renata Assis | RD | rpptoledo@rd.com.br |\n",
    "| 3 | Jose Roberto| RD | joroliverira@rd.com.br |\n",
    "| 4 | Deborah Foroni | InRad | deborah.foroni@hc.fm.usp.br|\n",
    "| 5 | Marisa Aguiar | Hospital Einstein | marisa.aguiar@einstein.br| \n",
    "| 6 | Edivaldo Nery | Prevent Senior| edivaldo.oliveira@preventsenior.com.br| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d709783e-b396-4638-808d-44c73729dce6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Databricks-BR/genai_hackathon/main/images/head_contexto.png\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6993a48-d46f-4c44-afae-194ca16319da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Cenário Atual \n",
    " Atualmente não existe um sistematica de conscientização de nossos clientes com relação aos cuidados com automedicação. \n",
    "\n",
    "</br></br>\n",
    "\n",
    "#### Dores / Necessidades do Negócio\n",
    "\n",
    "Melhorar a qualidade de vida, principalmente do público idoso, fornecendo uma ferramenta para orientar quanto ao perigo da automedicação ou seja ter interação medicamentosas.\n",
    "\n",
    "</br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "236c0cdf-6f4d-44d1-8389-e3a2709ef089",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Databricks-BR/genai_hackathon/main/images/head_proposito.png\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcf8f13d-429a-47d5-8add-78caa4114084",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Objetivo da Solução Proposta\n",
    "\n",
    "Entregar aos nossos clientes, no ato da compra de 2+ produtos, se existem interações medicamentosas(adversas ou não)\n",
    "caso os medicamentos sejam consumidos concomitantemente.\n",
    "</br></br>\n",
    "\n",
    "#### Benefícios\n",
    "\n",
    "Melhorar a qualidade de vida de nossos clientes, sobretudo idosos e/ou polimedicados asism como criar uma ferramenta de fidelização do cliente.\n",
    "</br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dce8f2d-87bf-4c35-8726-3402e793757b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Databricks-BR/genai_hackathon/main/images/head_arquitetura.png\" width=\"900px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "281c94be-8351-4e79-b0fe-4f74145ef466",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Arquitetura\n",
    "\n",
    "* Provedor Cloud:   Azure / AWS / GCP\n",
    "* Cluster / DB Runtime:   ____\n",
    "* Bibliotecas utilizadas: _____\n",
    "\n",
    "### Técnicas Utilizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab0a800d-4c05-4bba-a4b4-e6f841d35b39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Databricks-BR/genai_hackathon/main/images/head_referencias.png\" width=\"900px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46920e80-3f93-410b-b658-e1a56e74e7a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Referências:\n",
    "* [Introdução ao DBRX LLM foundation model Databricks](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm)\n",
    "* [Foundation LLM Models APIs](https://docs.databricks.com/en/machine-learning/foundation-models/index.html#pay-per-token-foundation-model-apis)\n",
    "* outros ...\n",
    "* outros ...\n",
    "* outros ...\n",
    "* outros ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95d45a6b-514c-4673-ad4a-d7b36f4a19ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Preparação dos dados para nosso Chatbot com RAG\n",
    "\n",
    "## Construindo e indexando nosso conhecimento com o Databricks Vector Search\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-managed-flow-1.png?raw=true\" style=\"float: right; width: 800px; margin-left: 10px\">\n",
    "\n",
    "Neste notebook, vamos ingerir as páginas da documentação da Datrabricks **(em Português)** e indexá-las com o Databricks Vector Search para ajudar nosso chatbot a fornecer respostas melhores.\n",
    "\n",
    "Preparar dados de alta qualidade é fundamental para o desempenho do seu chatbot. Recomendamos que você reserve um tempo para implementar essas etapas com seu próprio conjunto de dados.\n",
    "\n",
    "Felizmente, o Lakehouse AI oferece soluções de ponta para acelerar seus projetos de IA e LLMs, além de simplificar a ingestão e preparação de dados em escala.\n",
    "\n",
    "Para este exemplo, usaremos a documentação do Databricks em [docs.databricks.com/pt](docs.databricks.com/pt):\n",
    "- Download das páginas web\n",
    "- Dividir as páginas em pequenos trechos de texto\n",
    "- Calcular os embeddings usando um Databricks Foundation Model\n",
    "- Criar um índice no Databricks Vector Search\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=1444828305810485&notebook=%2F01-quickstart%2F01-Data-Preparation-and-Index&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F01-quickstart%2F01-Data-Preparation-and-Index&version=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98ce4073-ca1e-45b6-8d5b-6bacf7118098",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"/Workspace/Users/labuser6857446@vocareum.com/Captura de tela de 2024-06-05 15-49-51.png?raw=true\" style=\"float: right; width: 800px; margin-left: 10px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45e15cf6-0636-411b-87a2-8733865b715a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 0. Pré-requisitos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d1fb4d4-c373-4293-ab65-9d39c01a4aaf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Gerais\n",
    "\n",
    "1. Antes de seguir adiante, revise os requisitos no notebook **[./requirements.md]($./requirements.md)**\n",
    "1. Customize as configurações no notebook **[../config]($../config)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fffbf38-2416-4e53-bc45-39730f0a58ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Instalar as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfb2dffd-8ad5-4717-b738-33c2adc550cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nRequirement already satisfied: mlflow==2.10.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (2.10.1)\nRequirement already satisfied: lxml==4.9.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (4.9.3)\nRequirement already satisfied: transformers==4.30.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (4.30.2)\nRequirement already satisfied: langchain==0.1.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (0.1.5)\nRequirement already satisfied: databricks-vectorsearch==0.22 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (0.22)\nRequirement already satisfied: docker<8,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from mlflow==2.10.1) (7.1.0)\nRequirement already satisfied: pyarrow<16,>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (7.0.0)\nRequirement already satisfied: pytz<2024 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (2021.3)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (6.0)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (2.0.0)\nRequirement already satisfied: gitpython<4,>=2.1.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (3.1.27)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (3.19.4)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (2.27.1)\nRequirement already satisfied: numpy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (1.21.5)\nRequirement already satisfied: alembic!=1.10.0,<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from mlflow==2.10.1) (1.13.1)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (2.11.3)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (1.7.3)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (3.5.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (1.0.2)\nRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from mlflow==2.10.1) (7.1.0)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (0.4)\nRequirement already satisfied: markdown<4,>=3.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (3.3.4)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (1.4.2)\nRequirement already satisfied: querystring-parser<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from mlflow==2.10.1) (1.2.4)\nRequirement already satisfied: gunicorn<22 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (20.1.0)\nRequirement already satisfied: Flask<4 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (1.1.2+db1)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from mlflow==2.10.1) (2.0.30)\nRequirement already satisfied: packaging<24 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from mlflow==2.10.1) (23.2)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (0.4.2)\nRequirement already satisfied: databricks-cli<1,>=0.8.7 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (0.17.4)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (8.0.4)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from transformers==4.30.2) (0.23.3)\nRequirement already satisfied: safetensors>=0.3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from transformers==4.30.2) (0.4.3)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.9/site-packages (from transformers==4.30.2) (3.6.0)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.9/site-packages (from transformers==4.30.2) (2022.3.15)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /databricks/python3/lib/python3.9/site-packages (from transformers==4.30.2) (0.13.2)\nRequirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.9/site-packages (from transformers==4.30.2) (4.64.0)\nRequirement already satisfied: langchain-core<0.2,>=0.1.16 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain==0.1.5) (0.1.23)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain==0.1.5) (8.3.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain==0.1.5) (0.6.6)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain==0.1.5) (1.33)\nRequirement already satisfied: pydantic<3,>=1 in /databricks/python3/lib/python3.9/site-packages (from langchain==0.1.5) (1.10.2)\nRequirement already satisfied: langsmith<0.1,>=0.0.83 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain==0.1.5) (0.0.87)\nRequirement already satisfied: langchain-community<0.1,>=0.0.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain==0.1.5) (0.0.20)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain==0.1.5) (3.9.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain==0.1.5) (4.0.3)\nRequirement already satisfied: mlflow-skinny<3,>=2.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from databricks-vectorsearch==0.22) (2.13.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (1.4.1)\nRequirement already satisfied: yarl<2.0,>=1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (1.9.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (1.3.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (6.0.5)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (21.4.0)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.9/site-packages (from alembic!=1.10.0,<2->mlflow==2.10.1) (1.2.0)\nRequirement already satisfied: typing-extensions>=4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from alembic!=1.10.0,<2->mlflow==2.10.1) (4.12.1)\nRequirement already satisfied: tabulate>=0.7.7 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.10.1) (0.8.9)\nRequirement already satisfied: six>=1.10.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.10.1) (1.16.0)\nRequirement already satisfied: oauthlib>=3.1.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.10.1) (3.2.0)\nRequirement already satisfied: pyjwt>=1.7.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.10.1) (2.6.0)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.5) (0.9.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.5) (3.21.2)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.9/site-packages (from docker<8,>=4.0.0->mlflow==2.10.1) (1.26.9)\nRequirement already satisfied: itsdangerous>=0.24 in /databricks/python3/lib/python3.9/site-packages (from Flask<4->mlflow==2.10.1) (2.0.1)\nRequirement already satisfied: Werkzeug>=0.15 in /databricks/python3/lib/python3.9/site-packages (from Flask<4->mlflow==2.10.1) (2.0.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.9/site-packages (from gitpython<4,>=2.1.0->mlflow==2.10.1) (4.0.10)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow==2.10.1) (5.0.0)\nRequirement already satisfied: setuptools>=3.0 in /databricks/python3/lib/python3.9/site-packages (from gunicorn<22->mlflow==2.10.1) (61.2.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2024.6.0)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.9/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow==2.10.1) (3.7.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.9/site-packages (from Jinja2<4,>=2.11->mlflow==2.10.1) (2.0.1)\nRequirement already satisfied: jsonpointer>=1.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.5) (2.4)\nRequirement already satisfied: anyio<5,>=3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.16->langchain==0.1.5) (4.4.0)\nRequirement already satisfied: idna>=2.8 in /databricks/python3/lib/python3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain==0.1.5) (3.3)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain==0.1.5) (1.2.1)\nRequirement already satisfied: sniffio>=1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain==0.1.5) (1.3.1)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow==2.10.1) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow==2.10.1) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow==2.10.1) (1.3.2)\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow==2.10.1) (3.0.4)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow==2.10.1) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow==2.10.1) (9.0.1)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from mlflow-skinny<3,>=2.4.0->databricks-vectorsearch==0.22) (1.25.0)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from mlflow-skinny<3,>=2.4.0->databricks-vectorsearch==0.22) (5.3.3)\nRequirement already satisfied: opentelemetry-api<3,>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from mlflow-skinny<3,>=2.4.0->databricks-vectorsearch==0.22) (1.25.0)\nRequirement already satisfied: deprecated>=1.2.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from opentelemetry-api<3,>=1.0.0->mlflow-skinny<3,>=2.4.0->databricks-vectorsearch==0.22) (1.2.14)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.9/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.0.0->mlflow-skinny<3,>=2.4.0->databricks-vectorsearch==0.22) (1.12.1)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from opentelemetry-sdk<3,>=1.0.0->mlflow-skinny<3,>=2.4.0->databricks-vectorsearch==0.22) (0.46b0)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow==2.10.1) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow==2.10.1) (2021.10.8)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow==2.10.1) (2.2.0)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow==2.10.1) (1.1.1)\nRequirement already satisfied: greenlet!=0.4.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.10.1) (3.0.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.5) (0.4.3)\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install mlflow==2.10.1 lxml==4.9.3 transformers==4.30.2 langchain==0.1.5 databricks-vectorsearch==0.22\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "396bd1b4-49cd-443f-a67a-21a32fea821f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Inicializar recursos e catálogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f6122c6c-e04c-4e2e-9593-dd980791f136",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Notebook not found: Users/_resources/00-init. Notebooks can be specified via a relative path (./Notebook or ../folder/Notebook) or via an absolute path (/Abs/Path/to/Notebook). Make sure you are specifying the path correctly.\n\nStacktrace:\n  /Users/labuser6857446@vocareum.com/01-Data-Preparation-and-Index (1): python",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../_resources/00-init $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "408673e6-3492-4c3b-bc12-dacda1c6cf6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Conjunto de dados da documentação da Databricks\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-data-prep-1.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "Primeiro, vamos acessar nosso conjunto de dados bruto como uma tabela Delta Lake.\n",
    "\n",
    "Para esta demonstração, já baixamos as páginas da documentação do docs.databricks.com/pt e salvamos o conteúdo HTML.\n",
    "\n",
    "Agora, vamos utilizar o Delta Sharing para acessar esses dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbcde847-5000-4244-b18a-4c9f326dcc68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**<font color=\"red\">ANTES DE CONTINUAR, SIGA AS INSTRUÇÕES ABAIXO.</font>**\n",
    "\n",
    "## Configure o acesso ao dataset\n",
    "\n",
    "1. Peça a um dos intrutores para liberar o acesso ao dataset\n",
    "1. Acesse `Catalog` no menu principal à esquerda\n",
    "1. Acesse `Delta Sharing` > `Shared with me`\n",
    "1. Procure por `databricks-field-eng`\n",
    "1. Ao lado de `br-genai-hackathon`, clique em `Create catalog`\n",
    "1. Digite o nome `br-genai-hackathon` e clique em `Create`\n",
    "\n",
    "<img src=\"https://github.com/Databricks-BR/genai_hackathon/blob/main/images/sharing.gif?raw=true\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4b50676-21f5-425f-a1fb-0120e0d3e61c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM `br-genai-hackathon`.chatbot.raw_documentation LIMIT 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ac631cb-d78f-414b-9aa4-927e7a8e9837",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 2. Dividindo as páginas da documentação em pequenos trechos\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-data-prep-2.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "Os modelos de LLM possuem um comprimento máximo de contexto de entrada e você não poderá calcular embeddings para textos muito longos. Além disso, quanto maior for o comprimento do contexto, mais tempo o modelo levará para fornecer uma resposta.\n",
    "\n",
    "A preparação do documento é fundamental para o bom desempenho do seu modelo e existem várias estratégias dependendo do seu conjunto de dados:\n",
    "\n",
    "* Dividir o documento em pequenos trechos (parágrafo, h2...)\n",
    "* Truncar documentos para um comprimento fixo\n",
    "* O tamanho do trecho depende do seu conteúdo e de como você o usará para criar sua solicitação. Adicionar vários trechos pequenos de documentos em sua solicitação pode fornecer resultados diferentes do que enviar apenas um grande trecho\n",
    "* Dividir em trechos grandes e pedir a um modelo para resumi-los para acelerar a inferência\n",
    "* Criar vários agentes para avaliar cada documento maior em paralelo e pedir a um agente final para criar sua resposta..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0183be38-3cb6-4618-951e-e0d2a0071e06",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Divindo a documentação por seções da página web\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/chunk-window-size.png?raw=true\" style=\"float: right\" width=\"700px\">\n",
    "<br/>\n",
    "Nesta demonstração, temos artigos de documentação extensos, que são muito longos para serem usados como prompt para o nosso modelo.\n",
    "\n",
    "Não poderemos usar vários documentos como contexto RAG, pois eles excederiam nosso tamanho máximo de entrada. Alguns estudos recentes também sugerem que um tamanho de janela maior nem sempre é melhor, pois os LLMs parecem se concentrar no início e no final do prompt.\n",
    "\n",
    "No nosso caso, vamos dividir esses artigos entre as tags HTML h2, remover o HTML e garantir que cada trecho tenha menos de 500 tokens usando o LangChain. \n",
    "\n",
    "#### LLM Window Size e Tokenizer\n",
    "\n",
    "A mesma frase pode retornar diferentes tokens para modelos diferentes. Os LLMs são fornecidos com um `Tokenizer` que você pode usar para contar os tokens de uma determinada frase (geralmente mais do que o número de palavras) (veja a [documentação do Hugging Face](https://huggingface.co/docs/transformers/main/tokenizer_summary) ou [OpenAI](https://github.com/openai/tiktoken)).\n",
    "\n",
    "Certifique-se de que o tokenizer que você estará usando corresponda ao seu modelo. O Databricks DBRX Instruct usa o mesmo tokenizer que o GPT4. Vamos usar a biblioteca `transformers` para contar os tokens do DBRX Instruct com seu tokenizer. Isso também manterá o tamanho dos tokens do nosso documento abaixo do tamanho máximo dos embeddings (1024).\n",
    "\n",
    "<br/>\n",
    "<br style=\"clear: both\">\n",
    "<div style=\"background-color: #def2ff; padding: 15px;  border-radius: 30px; \">\n",
    "  <strong>Nota</strong><br/>\n",
    "  Lembre-se de que os seguintes passos são específicos para o seu conjunto de dados. Esta é uma parte crítica para construir um assistente RAG de sucesso.\n",
    "  <br/> Sempre reserve um tempo para revisar manualmente os trechos criados e garantir que façam sentido e contenham informações relevantes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c18c46b-389d-4c08-9c11-daee31818022",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2970205304977190>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext_splitter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer, OpenAIGPTTokenizer\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n",
       "\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n",
       "\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n",
       "\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n",
       "\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages/langchain/text_splitter.py:52\u001B[0m\n",
       "\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n",
       "\u001B[1;32m     33\u001B[0m     AbstractSet,\n",
       "\u001B[1;32m     34\u001B[0m     Any,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     48\u001B[0m     cast,\n",
       "\u001B[1;32m     49\u001B[0m )\n",
       "\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrequests\u001B[39;00m\n",
       "\u001B[0;32m---> 52\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocuments\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseDocumentTransformer, Document\n",
       "\u001B[1;32m     54\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n",
       "\u001B[1;32m     56\u001B[0m TS \u001B[38;5;241m=\u001B[39m TypeVar(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTS\u001B[39m\u001B[38;5;124m\"\u001B[39m, bound\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTextSplitter\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n",
       "\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n",
       "\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n",
       "\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n",
       "\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages/langchain_core/documents/__init__.py:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocuments\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Document\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocuments\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseDocumentTransformer\n",
       "\u001B[1;32m      4\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDocument\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBaseDocumentTransformer\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n",
       "\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n",
       "\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n",
       "\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n",
       "\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages/langchain_core/documents/base.py:9\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mload\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mserializable\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Serializable\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpydantic_v1\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Field\n",
       "\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mDocument\u001B[39;00m(Serializable):\n",
       "\u001B[1;32m     10\u001B[0m     \u001B[38;5;124;03m\"\"\"Class for storing a piece of text and associated metadata.\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m     12\u001B[0m     page_content: \u001B[38;5;28mstr\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/main.py:198\u001B[0m, in \u001B[0;36mpydantic.main.ModelMetaclass.__new__\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/fields.py:506\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField.infer\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/fields.py:436\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField.__init__\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/fields.py:552\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField.prepare\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/fields.py:668\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField._type_analysis\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.9/typing.py:835\u001B[0m, in \u001B[0;36m_SpecialGenericAlias.__subclasscheck__\u001B[0;34m(self, cls)\u001B[0m\n",
       "\u001B[1;32m    833\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28missubclass\u001B[39m(\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m__origin__, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__origin__)\n",
       "\u001B[1;32m    834\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mcls\u001B[39m, _GenericAlias):\n",
       "\u001B[0;32m--> 835\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43missubclass\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__origin__\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    836\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__subclasscheck__\u001B[39m(\u001B[38;5;28mcls\u001B[39m)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: issubclass() arg 1 must be a class"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2970205304977190>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext_splitter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer, OpenAIGPTTokenizer\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages/langchain/text_splitter.py:52\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     33\u001B[0m     AbstractSet,\n\u001B[1;32m     34\u001B[0m     Any,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m     cast,\n\u001B[1;32m     49\u001B[0m )\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrequests\u001B[39;00m\n\u001B[0;32m---> 52\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocuments\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseDocumentTransformer, Document\n\u001B[1;32m     54\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m     56\u001B[0m TS \u001B[38;5;241m=\u001B[39m TypeVar(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTS\u001B[39m\u001B[38;5;124m\"\u001B[39m, bound\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTextSplitter\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages/langchain_core/documents/__init__.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocuments\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Document\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocuments\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseDocumentTransformer\n\u001B[1;32m      4\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDocument\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBaseDocumentTransformer\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages/langchain_core/documents/base.py:9\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mload\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mserializable\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Serializable\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpydantic_v1\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Field\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mDocument\u001B[39;00m(Serializable):\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;124;03m\"\"\"Class for storing a piece of text and associated metadata.\"\"\"\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     page_content: \u001B[38;5;28mstr\u001B[39m\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/main.py:198\u001B[0m, in \u001B[0;36mpydantic.main.ModelMetaclass.__new__\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/fields.py:506\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField.infer\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/fields.py:436\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField.__init__\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/fields.py:552\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField.prepare\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/fields.py:668\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField._type_analysis\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32m/usr/lib/python3.9/typing.py:835\u001B[0m, in \u001B[0;36m_SpecialGenericAlias.__subclasscheck__\u001B[0;34m(self, cls)\u001B[0m\n\u001B[1;32m    833\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28missubclass\u001B[39m(\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m__origin__, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__origin__)\n\u001B[1;32m    834\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mcls\u001B[39m, _GenericAlias):\n\u001B[0;32m--> 835\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43missubclass\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__origin__\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    836\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__subclasscheck__\u001B[39m(\u001B[38;5;28mcls\u001B[39m)\n\n\u001B[0;31mTypeError\u001B[0m: issubclass() arg 1 must be a class",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: issubclass() arg 1 must be a class",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.text_splitter import HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer, OpenAIGPTTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ab0921b-28ed-4d9f-a449-169da94bb394",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_chunk_size = 500\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=max_chunk_size, chunk_overlap=50)\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=[(\"h2\", \"header2\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff365f9c-77e6-4ad9-9df8-eb6eeaa2472a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2970205304977171>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext_splitter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer, OpenAIGPTTokenizer\n",
       "\u001B[1;32m      4\u001B[0m max_chunk_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m500\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n",
       "\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n",
       "\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n",
       "\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n",
       "\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages/langchain/text_splitter.py:52\u001B[0m\n",
       "\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n",
       "\u001B[1;32m     33\u001B[0m     AbstractSet,\n",
       "\u001B[1;32m     34\u001B[0m     Any,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     48\u001B[0m     cast,\n",
       "\u001B[1;32m     49\u001B[0m )\n",
       "\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrequests\u001B[39;00m\n",
       "\u001B[0;32m---> 52\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocuments\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseDocumentTransformer, Document\n",
       "\u001B[1;32m     54\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n",
       "\u001B[1;32m     56\u001B[0m TS \u001B[38;5;241m=\u001B[39m TypeVar(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTS\u001B[39m\u001B[38;5;124m\"\u001B[39m, bound\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTextSplitter\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n",
       "\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n",
       "\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n",
       "\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n",
       "\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages/langchain_core/documents/__init__.py:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocuments\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Document\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocuments\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseDocumentTransformer\n",
       "\u001B[1;32m      4\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDocument\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBaseDocumentTransformer\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n",
       "\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n",
       "\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n",
       "\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n",
       "\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages/langchain_core/documents/base.py:9\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mload\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mserializable\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Serializable\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpydantic_v1\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Field\n",
       "\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mDocument\u001B[39;00m(Serializable):\n",
       "\u001B[1;32m     10\u001B[0m     \u001B[38;5;124;03m\"\"\"Class for storing a piece of text and associated metadata.\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m     12\u001B[0m     page_content: \u001B[38;5;28mstr\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/main.py:198\u001B[0m, in \u001B[0;36mpydantic.main.ModelMetaclass.__new__\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/fields.py:506\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField.infer\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/fields.py:436\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField.__init__\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/fields.py:552\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField.prepare\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/fields.py:668\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField._type_analysis\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.9/typing.py:835\u001B[0m, in \u001B[0;36m_SpecialGenericAlias.__subclasscheck__\u001B[0;34m(self, cls)\u001B[0m\n",
       "\u001B[1;32m    833\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28missubclass\u001B[39m(\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m__origin__, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__origin__)\n",
       "\u001B[1;32m    834\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mcls\u001B[39m, _GenericAlias):\n",
       "\u001B[0;32m--> 835\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43missubclass\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__origin__\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    836\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__subclasscheck__\u001B[39m(\u001B[38;5;28mcls\u001B[39m)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: issubclass() arg 1 must be a class"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2970205304977171>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext_splitter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer, OpenAIGPTTokenizer\n\u001B[1;32m      4\u001B[0m max_chunk_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m500\u001B[39m\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages/langchain/text_splitter.py:52\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     33\u001B[0m     AbstractSet,\n\u001B[1;32m     34\u001B[0m     Any,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m     cast,\n\u001B[1;32m     49\u001B[0m )\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrequests\u001B[39;00m\n\u001B[0;32m---> 52\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocuments\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseDocumentTransformer, Document\n\u001B[1;32m     54\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m     56\u001B[0m TS \u001B[38;5;241m=\u001B[39m TypeVar(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTS\u001B[39m\u001B[38;5;124m\"\u001B[39m, bound\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTextSplitter\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages/langchain_core/documents/__init__.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocuments\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Document\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocuments\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseDocumentTransformer\n\u001B[1;32m      4\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDocument\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBaseDocumentTransformer\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages/langchain_core/documents/base.py:9\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mload\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mserializable\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Serializable\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpydantic_v1\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Field\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mDocument\u001B[39;00m(Serializable):\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;124;03m\"\"\"Class for storing a piece of text and associated metadata.\"\"\"\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     page_content: \u001B[38;5;28mstr\u001B[39m\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/main.py:198\u001B[0m, in \u001B[0;36mpydantic.main.ModelMetaclass.__new__\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/fields.py:506\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField.infer\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/fields.py:436\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField.__init__\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/fields.py:552\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField.prepare\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pydantic/fields.py:668\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField._type_analysis\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32m/usr/lib/python3.9/typing.py:835\u001B[0m, in \u001B[0;36m_SpecialGenericAlias.__subclasscheck__\u001B[0;34m(self, cls)\u001B[0m\n\u001B[1;32m    833\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28missubclass\u001B[39m(\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m__origin__, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__origin__)\n\u001B[1;32m    834\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mcls\u001B[39m, _GenericAlias):\n\u001B[0;32m--> 835\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43missubclass\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__origin__\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    836\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__subclasscheck__\u001B[39m(\u001B[38;5;28mcls\u001B[39m)\n\n\u001B[0;31mTypeError\u001B[0m: issubclass() arg 1 must be a class",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: issubclass() arg 1 must be a class",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Divide por H2, mas combina trechos muito pequenos para evitar que fiquem muito pequenos \n",
    "def split_html_on_h2(html, min_chunk_size = 20, max_chunk_size=500):\n",
    "  if not html:\n",
    "      return []\n",
    "  h2_chunks = html_splitter.split_text(html)\n",
    "  chunks = []\n",
    "  previous_chunk = \"\"\n",
    "  # Combina trechos muito pequenos para evitar que fiquem muito pequenos\n",
    "  for c in h2_chunks:\n",
    "    # Concatena o h2 (nota: poderíamos remover o trecho anterior para evitar duplicações)\n",
    "    content = c.metadata.get('header2', \"\") + \"\\n\" + c.page_content\n",
    "    if len(tokenizer.encode(previous_chunk + content)) <= max_chunk_size/2:\n",
    "        previous_chunk += content + \"\\n\"\n",
    "    else:\n",
    "        chunks.extend(text_splitter.split_text(previous_chunk.strip()))\n",
    "        previous_chunk = content + \"\\n\"\n",
    "  if previous_chunk:\n",
    "      chunks.extend(text_splitter.split_text(previous_chunk.strip()))\n",
    "  # Discarta trechos muito pequenos\n",
    "  return [c for c in chunks if len(tokenizer.encode(c)) > min_chunk_size]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73f3cb83-5bc9-4b71-a88b-fc320fee1f3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Vamos avaliar a função de divisão dos trechos\n",
    "html = spark.table(\"`br-genai-hackathon`.chatbot.raw_documentation\").limit(1).collect()[0]['text']\n",
    "split_html_on_h2(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36405ea1-f6bf-4d64-bf08-1b83728b625d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Criando os trechos e salvando-os em uma tabela Delta\n",
    "\n",
    "O último passo é aplicar nossa UDF em todo o texto da documentação e salvar os trechos extraídos na tabela `databricks_documentation`.\n",
    "\n",
    "*Observe que esta parte geralmente seria configurada como um job de produção, sendo executado assim que uma nova página de documentação for atualizada. <br/> Isso poderia ser configurado como um pipeline de Delta Live Table para consumir atualizações incrementalmente.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2698e2a2-3de5-427c-b719-65a405d05e01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Note que o Change Data Feed foi habilitado na tabela para permitir a alimentação incremental do índice\n",
    "CREATE TABLE IF NOT EXISTS databricks_documentation (\n",
    "  id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  url STRING,\n",
    "  content STRING\n",
    ") TBLPROPERTIES (delta.enableChangeDataFeed = true); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5aff0830-e2a0-4e92-97d2-157d7c3ab8d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cria uma UDF para dividir os trechos de toda a documentação com Spark\n",
    "@pandas_udf(\"array<string>\")\n",
    "def parse_and_split(docs: pd.Series) -> pd.Series:\n",
    "    return docs.apply(split_html_on_h2)\n",
    "    \n",
    "(spark.table(\"`br-genai-hackathon`.chatbot.raw_documentation\")\n",
    "      .filter('text is not null')\n",
    "      .withColumn('content', F.explode(parse_and_split('text')))\n",
    "      .drop(\"text\")\n",
    "      .write.mode('overwrite').saveAsTable(\"databricks_documentation\"))\n",
    "\n",
    "display(spark.table(\"databricks_documentation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcab1504-a5ec-488e-ab64-8780d091be57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Indexando a documentação no Databricks Vector Search\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/databricks-vector-search-managed-type.png?raw=true\" style=\"float: right\" width=\"800px\">\n",
    "\n",
    "A Databricks fornece múltiplos tipos de índices no Vector Search:\n",
    "\n",
    "- **Managed embeddings**: você fornece uma coluna de texto e o nome do endpoint, e o Databricks sincroniza o índice com a sua tabela Delta\n",
    "- **Self Managed embeddings**: você calcula os embeddings e os salva como um campo da sua tabela Delta, o Databricks então sincronizará o índice\n",
    "- **Direct index**: quando você quer usar e atualizar seu índice sem uma tabela Delta\n",
    "\n",
    "Nesta demonstração, mostraremos como configurar um índice de **Managed Embeddings**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7b568c7-6623-46c8-857e-4c369cc146a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Databricks BGE Embeddings Foundation Model endpoints\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-data-prep-4.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "Foundation Models são fornecidos pela Databricks e podem ser utilizados imediatamente.\n",
    "\n",
    "O Databricks suporta vários tipos de endpoints para calcular embeddings ou inferir com um modelo:\n",
    "* **Foundation Model endpoint**: fornecido pela Databricks (ex: llama2-70B, MPT, BGE). **Utilizaremos esse**.\n",
    "* **External endpoint**: atua como um gateway para um modelo externo (ex: Azure OpenAI)\n",
    "* **Modelos customizados**: modelo refinado servido pelo Databricks Model Serving\n",
    "\n",
    "Abra a página [Model Serving](/ml/endpoints) para explorar e experimentar os Foundation Models.\n",
    "\n",
    "Para esta demonstração, usaremos os foundation models `BGE` (embeddings) e `llama2-70B` (chat). <br/><br/>\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/databricks-foundation-models.png?raw=true\" width=\"600px\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "980569f8-a718-4926-a2c4-fda3639c36c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.deployments\n",
    "deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "# Endpoints de Embeddings convertem texto em vetores. Segue um exemplo usando o BGE:\n",
    "response = deploy_client.predict(endpoint=\"databricks-bge-large-en\", inputs={\"input\": [\"What is Apache Spark?\"]})\n",
    "embeddings = [e['embedding'] for e in response.data]\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "646248df-f62f-4c7e-a47b-f7d5c4b0269a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Criando um Vector Search endpoint\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-data-prep-3.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "A função do **Vector Search endpoint** é servir os embeddings indexados (você pode pensar nele como um endpoint de API).\n",
    "\n",
    "Múltiplos índices podem usar o mesmo endpoint.\n",
    "\n",
    "Vamos começar criando um."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "507527a2-5aed-43d9-be59-e7506995e1f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "if not endpoint_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME):\n",
    "    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type=\"STANDARD\")\n",
    "\n",
    "wait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "print(f\"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e0d00f8-0873-47bd-a4ff-2263a9c21b5f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Você pode ver os endpoint criados na página do [Vector Search](/compute/vector-search). Clique no nome do endpoint para ver todos os índices servidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98a9fddc-16c6-44b3-8243-ead6f59c6a43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Criando um índice com Managed Embeddings e BGE\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/index_creation.gif?raw=true\" width=\"600px\" style=\"float: right\">\n",
    "\n",
    "Com o Managed Embeddings, o Databricks calculará automaticamente os embeddings para nós. Este é o modo mais fácil de começar com o Databricks.\n",
    "\n",
    "Agora só precisamos pedir ao Databricks para criar o índice.\n",
    "\n",
    "Só precisamos especificar a coluna de texto e nosso modelo de embeddings (`BGE`) para que o Databricks calcule os embeddings automaticamente para nós.\n",
    "\n",
    "Isso pode ser feito usando a API ou em alguns cliques dentro do Unity Catalog Explorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7b0f739-9e90-4bd2-adc7-1c1aee984127",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import databricks.sdk.service.catalog as c\n",
    "\n",
    "# A tabela a ser indexada\n",
    "source_table_fullname = f\"{catalog}.{db}.databricks_documentation\"\n",
    "# Onde armazanaremos o índice\n",
    "vs_index_fullname = f\"{catalog}.{db}.databricks_documentation_vs_index\"\n",
    "\n",
    "if not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):\n",
    "  print(f\"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...\")\n",
    "  vsc.create_delta_sync_index(\n",
    "    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    index_name=vs_index_fullname,\n",
    "    source_table_name=source_table_fullname,\n",
    "    pipeline_type=\"TRIGGERED\",\n",
    "    primary_key=\"id\",\n",
    "    embedding_source_column='content',\n",
    "    embedding_model_endpoint_name='databricks-bge-large-en'\n",
    "  )\n",
    "  # Espera que o índice esteja pronto e que todos os embeddings tenham sido criados\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "else:\n",
    "  # Dispara a atualização dos nossos embeddings com os novos dados salvos na tabela\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "  vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).sync()\n",
    "\n",
    "print(f\"index {vs_index_fullname} on table {source_table_fullname} is ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74b4f037-633e-4e47-8457-70e71fc1f460",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4. Testando a busca por conteúdos similares\n",
    "\n",
    "Isso é tudo que precisamos fazer. O Databricks irá capturar e sincronizar automaticamente novas entradas na sua Delta Live Table.\n",
    "\n",
    "Observe que, dependendo do tamanho do seu conjunto de dados e do modelo, a criação do índice pode levar alguns segundos para iniciar e indexar seus embeddings.\n",
    "\n",
    "Vamos experimentar e buscar por conteúdos similares.\n",
    "\n",
    "*Nota: `similarity_search` também possui um parâmetro de filtro. Isso é útil para adicionar uma camada de segurança ao seu sistema RAG: você pode filtrar algum conteúdo sensível com base em quem está fazendo a chamada (por exemplo, filtrar por um departamento específico com base na preferência do usuário).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0aa65503-8109-45f6-b951-a93ad9c04bd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import mlflow.deployments\n",
    "# deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "question = \"O que é Delta Live Tables?\"\n",
    "\n",
    "results = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).similarity_search(\n",
    "  query_text=question,\n",
    "  columns=[\"url\", \"content\"],\n",
    "  num_results=1)\n",
    "docs = results.get('result', {}).get('data_array', [])\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "308e5b28-d626-4dcc-80db-cf48611b9ba4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Próximo passo: Construir nosso Chatbot com RAG\n",
    "\n",
    "Vimos como o Databricks Lakehouse AI facilita a ingestão e preparação dos seus documentos, e como é possível implantar um índice do Vector Search em cima deles com apenas algumas linhas de código.\n",
    "\n",
    "Isso simplifica e acelera seus projetos de dados, permitindo que você se concentre no próximo passo: criar o endpoint do seu chatbot em tempo real com uma boa estratégia de RAG.\n",
    "\n",
    "Abra o notebook [02-Deploy-RAG-Chatbot-Model]($./02-Deploy-RAG-Chatbot-Model) para criar e operacionalizar seu chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "316e886a-0b03-469b-bafd-2accfd2ab8c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nRequirement already satisfied: mlflow==2.10.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (2.10.1)\nRequirement already satisfied: lxml==4.9.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (4.9.3)\nRequirement already satisfied: transformers==4.30.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (4.30.2)\nRequirement already satisfied: langchain==0.1.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (0.1.5)\nRequirement already satisfied: databricks-vectorsearch==0.22 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (0.22)\nRequirement already satisfied: docker<8,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from mlflow==2.10.1) (7.1.0)\nRequirement already satisfied: pyarrow<16,>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (7.0.0)\nRequirement already satisfied: pytz<2024 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (2021.3)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (6.0)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (2.0.0)\nRequirement already satisfied: gitpython<4,>=2.1.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (3.1.27)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (3.19.4)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (2.27.1)\nRequirement already satisfied: numpy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (1.21.5)\nRequirement already satisfied: alembic!=1.10.0,<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from mlflow==2.10.1) (1.13.1)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (2.11.3)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (1.7.3)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (3.5.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (1.0.2)\nRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from mlflow==2.10.1) (7.1.0)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (0.4)\nRequirement already satisfied: markdown<4,>=3.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (3.3.4)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (1.4.2)\nRequirement already satisfied: querystring-parser<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from mlflow==2.10.1) (1.2.4)\nRequirement already satisfied: gunicorn<22 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (20.1.0)\nRequirement already satisfied: Flask<4 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (1.1.2+db1)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from mlflow==2.10.1) (2.0.30)\nRequirement already satisfied: packaging<24 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from mlflow==2.10.1) (23.2)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (0.4.2)\nRequirement already satisfied: databricks-cli<1,>=0.8.7 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (0.17.4)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow==2.10.1) (8.0.4)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from transformers==4.30.2) (0.23.3)\nRequirement already satisfied: safetensors>=0.3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from transformers==4.30.2) (0.4.3)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.9/site-packages (from transformers==4.30.2) (3.6.0)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.9/site-packages (from transformers==4.30.2) (2022.3.15)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /databricks/python3/lib/python3.9/site-packages (from transformers==4.30.2) (0.13.2)\nRequirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.9/site-packages (from transformers==4.30.2) (4.64.0)\nRequirement already satisfied: langchain-core<0.2,>=0.1.16 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain==0.1.5) (0.1.23)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain==0.1.5) (8.3.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain==0.1.5) (0.6.6)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain==0.1.5) (1.33)\nRequirement already satisfied: pydantic<3,>=1 in /databricks/python3/lib/python3.9/site-packages (from langchain==0.1.5) (1.10.2)\nRequirement already satisfied: langsmith<0.1,>=0.0.83 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain==0.1.5) (0.0.87)\nRequirement already satisfied: langchain-community<0.1,>=0.0.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain==0.1.5) (0.0.20)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain==0.1.5) (3.9.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain==0.1.5) (4.0.3)\nRequirement already satisfied: mlflow-skinny<3,>=2.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from databricks-vectorsearch==0.22) (2.13.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (1.4.1)\nRequirement already satisfied: yarl<2.0,>=1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (1.9.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (1.3.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (6.0.5)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (21.4.0)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.9/site-packages (from alembic!=1.10.0,<2->mlflow==2.10.1) (1.2.0)\nRequirement already satisfied: typing-extensions>=4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from alembic!=1.10.0,<2->mlflow==2.10.1) (4.12.1)\nRequirement already satisfied: tabulate>=0.7.7 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.10.1) (0.8.9)\nRequirement already satisfied: six>=1.10.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.10.1) (1.16.0)\nRequirement already satisfied: oauthlib>=3.1.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.10.1) (3.2.0)\nRequirement already satisfied: pyjwt>=1.7.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.10.1) (2.6.0)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.5) (0.9.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.5) (3.21.2)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.9/site-packages (from docker<8,>=4.0.0->mlflow==2.10.1) (1.26.9)\nRequirement already satisfied: itsdangerous>=0.24 in /databricks/python3/lib/python3.9/site-packages (from Flask<4->mlflow==2.10.1) (2.0.1)\nRequirement already satisfied: Werkzeug>=0.15 in /databricks/python3/lib/python3.9/site-packages (from Flask<4->mlflow==2.10.1) (2.0.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.9/site-packages (from gitpython<4,>=2.1.0->mlflow==2.10.1) (4.0.10)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow==2.10.1) (5.0.0)\nRequirement already satisfied: setuptools>=3.0 in /databricks/python3/lib/python3.9/site-packages (from gunicorn<22->mlflow==2.10.1) (61.2.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2024.6.0)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.9/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow==2.10.1) (3.7.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.9/site-packages (from Jinja2<4,>=2.11->mlflow==2.10.1) (2.0.1)\nRequirement already satisfied: jsonpointer>=1.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.5) (2.4)\nRequirement already satisfied: anyio<5,>=3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.16->langchain==0.1.5) (4.4.0)\nRequirement already satisfied: idna>=2.8 in /databricks/python3/lib/python3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain==0.1.5) (3.3)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain==0.1.5) (1.2.1)\nRequirement already satisfied: sniffio>=1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain==0.1.5) (1.3.1)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow==2.10.1) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow==2.10.1) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow==2.10.1) (1.3.2)\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow==2.10.1) (3.0.4)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow==2.10.1) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow==2.10.1) (9.0.1)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from mlflow-skinny<3,>=2.4.0->databricks-vectorsearch==0.22) (1.25.0)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from mlflow-skinny<3,>=2.4.0->databricks-vectorsearch==0.22) (5.3.3)\nRequirement already satisfied: opentelemetry-api<3,>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from mlflow-skinny<3,>=2.4.0->databricks-vectorsearch==0.22) (1.25.0)\nRequirement already satisfied: deprecated>=1.2.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from opentelemetry-api<3,>=1.0.0->mlflow-skinny<3,>=2.4.0->databricks-vectorsearch==0.22) (1.2.14)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.9/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.0.0->mlflow-skinny<3,>=2.4.0->databricks-vectorsearch==0.22) (1.12.1)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from opentelemetry-sdk<3,>=1.0.0->mlflow-skinny<3,>=2.4.0->databricks-vectorsearch==0.22) (0.46b0)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow==2.10.1) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow==2.10.1) (2021.10.8)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow==2.10.1) (2.2.0)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow==2.10.1) (1.1.1)\nRequirement already satisfied: greenlet!=0.4.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.10.1) (3.0.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.5) (0.4.3)\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install mlflow==2.10.1 lxml==4.9.3 transformers==4.30.2 langchain==0.1.5 databricks-vectorsearch==0.22\n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48f35a1a-da19-4b5a-a214-94d2ee1249bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nRequirement already satisfied: langchain in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (0.1.5)\nCollecting langchain\n  Downloading langchain-0.2.2-py3-none-any.whl (973 kB)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0\n  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\nCollecting langchain-core<0.3.0,>=0.2.0\n  Downloading langchain_core-0.2.4-py3-none-any.whl (310 kB)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain) (8.3.0)\nRequirement already satisfied: PyYAML>=5.3 in /databricks/python3/lib/python3.9/site-packages (from langchain) (6.0)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain) (2.0.30)\nRequirement already satisfied: numpy<2,>=1 in /databricks/python3/lib/python3.9/site-packages (from langchain) (1.21.5)\nRequirement already satisfied: pydantic<3,>=1 in /databricks/python3/lib/python3.9/site-packages (from langchain) (1.10.2)\nCollecting langsmith<0.2.0,>=0.1.17\n  Downloading langsmith-0.1.72-py3-none-any.whl (124 kB)\nRequirement already satisfied: requests<3,>=2 in /databricks/python3/lib/python3.9/site-packages (from langchain) (2.27.1)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain) (3.9.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: yarl<2.0,>=1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.4.0)\nRequirement already satisfied: packaging<24.0,>=23.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\nRequirement already satisfied: jsonpointer>=1.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\nCollecting orjson<4.0.0,>=3.9.14\n  Downloading orjson-3.10.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\nRequirement already satisfied: typing-extensions>=4.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (4.12.1)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2021.10.8)\nRequirement already satisfied: greenlet!=0.4.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nInstalling collected packages: orjson, langsmith, langchain-core, langchain-text-splitters, langchain\n  Attempting uninstall: langsmith\n    Found existing installation: langsmith 0.0.87\n    Uninstalling langsmith-0.0.87:\n      Successfully uninstalled langsmith-0.0.87\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.1.23\n    Uninstalling langchain-core-0.1.23:\n      Successfully uninstalled langchain-core-0.1.23\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.1.5\n    Uninstalling langchain-0.1.5:\n      Successfully uninstalled langchain-0.1.5\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain-community 0.0.20 requires langchain-core<0.2,>=0.1.21, but you have langchain-core 0.2.4 which is incompatible.\nlangchain-community 0.0.20 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.1.72 which is incompatible.\nSuccessfully installed langchain-0.2.2 langchain-core-0.2.4 langchain-text-splitters-0.2.1 langsmith-0.1.72 orjson-3.10.3\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cee9602b-c773-45c1-badc-42c5f2f09689",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "import pandas as pd\n",
    "\n",
    "df = spark.sql(\"SELECT * FROM `_imnt_catalog`.`default`.`tb_medicamentos_100`\")\n",
    "df_pandas = df.toPandas()\n",
    "\n",
    "df_pandas[\"join_columns\"] = df_pandas[\"ds_principio_ativo\"] + \" \" + df_pandas[\"ds_interacao_principio_ativo\"] + \" \" + df_pandas[\"ds_tipo_severidade\"] +  \" \" + df_pandas[\"ds_descricao_extendida_interacao\"]\n",
    "\n",
    "txt = df_pandas.to_dict(\"records\")\n",
    "df_vector = pd.DataFrame({\"vector_column\":txt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5558afcc-0e54-49f8-8f98-be016e001dc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer, OpenAIGPTTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8446053-ae25-4fff-b1fc-c74ece86b5c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\nVersion: 0.2.2\nSummary: Building applications with LLMs through composability\nHome-page: https://github.com/langchain-ai/langchain\nAuthor: \nAuthor-email: \nLicense: MIT\nLocation: /local_disk0/.ephemeral_nfs/envs/pythonEnv-0a1c4193-37f4-4edc-a651-2a09c9f174bb/lib/python3.9/site-packages\nRequires: langchain-text-splitters, langchain-core, tenacity, PyYAML, SQLAlchemy, numpy, pydantic, langsmith, requests, aiohttp, async-timeout\nRequired-by: \n"
     ]
    }
   ],
   "source": [
    "%pip show langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba110a56-4051-4b68-95b8-4058b1054690",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-Data-Preparation-and-Index (1)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
